const x = {"files": {"storage.md": {"content": "# kachery Storage Design\n\nThe purpose of the kachery system is to facilitate transferring data\nbetween network peers. kachery achieves this purpose by\nproviding a universally consistent name for each record stored in the\nsystem, and storing these records in a sophisticated\nhierarchy of *caches* of varying size and\naccessibility at several different levels in the tool.\n\n(Incidentally, this hierarchy of caches is what gives kachery its name!)\n\nIn this page, the term \"file\" is used to refer specifically to the\n[content-addressable information type](./Overview.md#What-It-Does) within\nkachery. The term \"record\" will be used when referring to a file on a\nfilesystem (which could store any of the three types of information).\n\n## Content-Addressable Storage\n\nStoring and retrieving information consistently across different systems\nrequires a consistent reference for each record. At its heart, kachery\nprovides this through *content-addressable storage*: files are stored\nin the kachery system according to a directory structure based on the\nSHA1 hash of the file's contents.\n\nBecause files are content-addressable, their official location will change whenever\ntheir content changes. This can be a convenient, if very inefficient, means of creating\na change log for files; however, more realistically it is advisable not to store\nfiles in kachery if they are expected to undergo frequent changes. Store files when\nthey've reached their final form.\n\n### Feeds\n\nUnlike files, *feeds* are supposed to change. In order to provide each feed with a\nconsistent location, feeds are identified by an arbitrary character string (which\nis actually the public part of a\n[public-private key pair](https://en.wikipedia.org/wiki/Public-key_cryptography)).\nThe node which owns the feed stores the corresponding private key, and uses\nit to sign each message that is approved to be added to the feed. See the\n[documentation on feeds](./feeds.md) and on the\n[feed signature security model](./feeds.md#Feeds) for more details.\n\n### Tasks\n\nSimilarly, tasks cannot be stored according to their content: if we had the\ncontent, there would be no need to run the task! Instead, tasks are identified\nby a fingerprint composed of the registered task name and the parameters\npassed to it (in JSON-serialized format). For more information, see the\n[documentation on tasks](./tasks.md).\n\n## Storage Hierarchy\n\nDepending on configuration, there are up to three different caches per node in a\ntypical kachery use case.\n\nFirst is the shortest-term cache, the cloud storage cache. Individual nodes\ntransfer data by uploading it to this storage space. For reasons of economy,\nthis space is expected to be limited and some information will be cleared out\nof this cache when appropriate; however, it provides ready access to all nodes,\neven if the original provider of the information is not presently online.\n\nSecond, each node is expected to have local storage: this is where data goes\nwhen the user has added local information to kachery, and where data retrieved\nfrom the kachery network is stored. Because this storage is part of a filesystem\non user-owned hardware, it can be scaled cheaply and information can be\nstored for as long as desired.\n\nFinally, in the event that the kachery client is running on a separate\nmachine from the kachery server, the client machine can also be configured\nwith a local cache. This cache will be used when the client is in \"offline\"\nmode (not connected to a node). **AS WELL AS XYZ?**\n\n### Organization of data within local storage\n\nThe root of the information records stored in any particular kachery node's\ncache is the directory identified by the `KACHERY_STORAGE_DIR` environment\nvariable. (If this is not set, it defaults to a directory named\n`kachery-storage` in the home directory of whatever user is running the\n`kachery-daemon` process on the node.)\n\nWithin this directory, there are separate subdirectories for configuration,\nas well as for each of the different information types.\n\n* **Files** follow the content-addressability rules: they are stored according\nto the value of the SHA1 fingerprint of the file's contents. These files are\nstored in the `sha1` subdirectory of `$KACHERY_STORAGE_DIR`.\n\n  For example, the file\n  [studysets.json](https://github.com/flatironinstitute/spikeforest_recordings/blob/master/recordings/studysets),\n  a JSON file which describes the recordings in\n  [SpikeForest](http://spikeforest.flatironinstitute.org/), is stored in kachery\n  with the URI:\n  > `sha1://f728d5bf1118a8c6e2dfee7c99efb0256246d1d3/studysets.json`\n  \n  This URI, with the `sha1://` prefix, is used to retrieve the data when making\n  requests through the [kachery client](./client-howto.md). The URI preserves the\n  name of the originally stored file (`studysets.json`), as well as the `sha1`\n  hash of its contents (`f728d5...`).\n  \n  On any system where the file is stored, the actual file will be stored within the\n  local kachery storage directory under the path:\n  > `sha1/f7/28/d5/f728d5bf1118a8c6e2dfee7c99efb0256246d1d3`\n  The first three pairs of hexadecimal characters in the hash are used to create a\n  three-level directory tree, which distributes the stored files across the\n  filesystem and avoids storing unduly large numbers of files within any single directory.\n\n* **Feeds** are stored according to their Feed ID within the `feeds` subdirectory\nof `$KACHERY_STORAGE_DIR`. This directory has its own three-level hierarchy of\ntwo-hexadecimal-character subdirectories. The full feed ID is used as the name of\na directory here, and that directory contains a `subfeeds` directory which has\na further three-level hierarchy, at the base of which are the messages for each\nsubfeed. (See\n[the documentation on feeds](feeds.md) for more information about feeds and subfeeds.)\nFinally, within the subfeed directory, messages are stored in a record called `messages`.\n\n  As an example, the subfeed:\n  > `b28b7af69320201d1cf206ebf28373980add1451`\n\n  belonging to the feed:\n  > `021791fd201ac1e4e5efd3cc461ff8fda03bba130ffb7d48e7be94b7860bbc1c`\n\n  has its messages documented in a record at:\n  > `$KACHERY_STORAGE_DIR/feeds/02/17/91/021791fd[...]bbc1c/subfeeds/b2/8b/7a/b28b7a[...]d1451/messages`\n\n  (where ellipses have been added for readability--the full hex strings are used in the actual system).\n\n* Finally, **task results** are stored in `$KACHERY_STORAGE_DIR/job-cache` with a similar\nthree-level two-hex-character directory structure. Tasks are keyed according to a fingerprint\nderived from the task name and its (serialized) parameters.\n\n### Local data storage access\n\nLocal kachery storage is located on the file system of the host running the\nnode (or network-attached storage suitably mounted). Access will be governed\nby the host operating system/file system permissions. Because the file system\nis not aware of kachery network channels, channel permissions will not be\nenforced at this level. This is unlikely to matter locally for the vast majority of\nuse cases, since we assume that a system user with access to the kachery store\nvia the file system would also have access to the client command line. However,\na node with membership in multiple channels could potentially share files from\none channel with nodes from another channel:\n\n> Suppose Node X has `REQUEST-FILE`\npermission on Channel A and `PROVIDE-FILE` permission on Channel B. Node Y has\n`REQUEST-FILE` permission on Channel B and requests file `SHA1://12345`,\nwhich Node X has already downloaded from a source on Channel A. The store does\nnot track the channel-of-origin of this file, so Node X would provide the file\nto Node Y.\n\nFor this to work, a user of Node Y has to know the SHA1 of the desired file, and\nsuspect that there are nodes on the channel which are also members of another\nchannel where this file is shared. This is unlikely, but possible.\n\nTo mitigate this risk, it is important for node owners to be responsible about\nthe channels they join.\n\n### Organization of data on the cloud resource\n\nUnlike the node- or client-local caches, cloud storage systems are different from\nconventional filesystems in that they\n[do not typically have a concept of directories](https://cloud.google.com/storage/docs/folders).\nInstead, objects are referred to solely by URL. Fortunately, this maps perfectly with\nthe content-addressable design of kachery, in which every record is assigned a unique\nURL identifier anyway.\n\nIn short, the data on the cloud storage cache has no particular organization, but\nthe kachery system is not concerned with this in any way.\n\n### Splitting Files into Pages\n\nTo improve the speed and robustness of data transfer, records (with any kind of information)\nover a threshold size are split into multiple chunks, or *pages*, which are stored as discrete\nunits within the kachery storage hierarchy. The overall data record includes a *manifest*\nthat tells kachery how to put the pieces back together. With this design, it is possible\nto transfer a record through multiple simultaneous connections, and easy to resume an interrupted\npartial download with minimal need for redundant downloading. This process is\ntransparent to the kachery user.\n\nAt present, the threshold size is 20 MB, but **IS THERE A SETTING OR CUSTOMIZATION OR ANYTHING?**\n"}, "network.md": {"content": "# kachery network\n\n![Kachery network](https://docs.google.com/drawings/d/e/2PACX-1vQUnokzwrFHdIO-LjloBjHGbOHE7uaLEh9frzx-WrJbn_z0lIScFhyNWCBYZfj6ofjNHRoJbzjJbFlS/pub?w=960&h=720)\n\nThe kachery network is organized into nodes and channels, and each node may belong to one or more channels configured with particular roles.\n\n* A **[node](./node.md)** is usually a running instance of the [kachery daemon](https://github.com/kacheryhub/kachery-daemon) software: think of it as a computer\nwhose user would like to send or receive files. But a web app or other cloud-deployed entity can also serve as a kachery node. Each node has a unique node ID (the public part of a public/private key pair), an owner ID, and a channel membership configuration. Communication and data transfer on the network takes\nplace between nodes via channels.\n\n* A **[channel](./channel.md)** defines a community of nodes that exchange files: the channel has\na unique identifier/name and a particular owner who can control access to it. It also\ndefines associated cloud storage and pub-sub communication resources which allow caching of files and task results and facilitate data transfer and interaction between nodes.\n\n* **[kacheryhub](./hub.md)** is the central website where a user\ncan create or manage channels, configure nodes, and where a channel owner\ncan control which nodes have permission to connect to the channel\nand what actions they are allowed to take in the channel. This site also acts as a *federated authentication provider* for granting on-demand access to nodes for cloud resources associated with channels.\n\n* **[Storage buckets](./storage-bucket.md)** are cloud resources associated with channels and are used as both a cache and as a [bus](https://en.wikipedia.org/wiki/Bus_(computing))\nover which data is transferred between nodes (nodes do not interact directly with each other). For now kachery exclusively uses\n[Google Cloud Storage Buckets](https://cloud.google.com/storage/docs/creating-buckets)), but in the future it will be possible to use AWS S3 buckets or other custom object storage solutions.\n\n* **[Publish/subscribe services](./pub-sub.md)** are used for low-latency communication between nodes. Much the same as nodes use a cloud storage\nbucket to transfer files, they also communicate requests and status messages\nthrough shared publish/subscribe (pub-sub) message channels. This is presently implemented\nthrough [Ably](https://ably.com/pub-sub-messaging). A node requesting\na file will send a message to the corresponding pub-sub channel,\nand nodes with file-uploading permissions listen for requests and respond by uploading\nthe file to the cloud storage space.\n\n* **[Python scripts](./sharing-data.md)** on a workstation running a node daemon use the `kachery-client` Python package to interact with the network through the daemon. Scripts can request to load data objects (e.g. Numpy arrays or picked Python objects), files, feeds, or task results, which are downloaded from the channel and loaded into Python variables. Scripts can also store files or results to the local kachery storage, which then makes those data available to other authorized nodes on the network (provided that the URIs are known to the other nodes).\n\n* One or more **[task backends](./tasks.md)** can be run on a workstation node in order to make computations or actions available to other authorized nodes in the kachery network (usually a web app node). In order for a remote node to request that a task be run, it must have task-requesting permissionn on a channel one which the task-running node has the task-providing configuration.\n\n* Files and feeds are always stored first in **[local node storage](./local-node-storage.md)** and are only uploaded to a channel if and when that file is requested by an authorized node.\n\nSee also:\n\n* [Kachery security model](./security.md)\n\n"}, "local-node-storage.md": {"content": "# Local node storage\n\nKachery [files](./content-uris.md) and [feeds](./feeds.md) are first stored locally and are uploaded to the [kachery network](./network.md) only when requested by another [node](./node.md) with the appropriate access permissions.\n\nFor example, when we run the following script,\n\n```python\nimport kachery_client as kc\n\nuri = kc.store_text('example-text')\nprint(uri)\n# Outputs: sha1://5e8fe2d0b1e93be61186fda5a9021ceb89b07326/file.txt\n```\n\na text file is stored locally at:\n\n```bash\n# Note: if KACHERY_STORAGE_DIR is not set at the time the kachery daemon is run, the default path ~/kachery-storage will be used\n$KACHERY_STORAGE_DIR/sha1/5e/8f/e2/5e8fe2d0b1e93be61186fda5a9021ceb89b07326\n```\n\nand will only be uploaded to the cloud only if (a) this file is requested by another node who belongs to some channel `C`, (b) the remote node is configured to request files on `C`, and (c) our node is configured to provide files on `C`.\n\nSimilarly, any feeds we create locally will first be stored in a local SQLite database at:\n\n```bash\n$KACHERY_STORAGE_DIR/feeds.db\n```\n\nand will only be uploaded to the cloud if another authorized node requests the feed on a channel, and we are configured to provide feeds on that channel.\n\n"}, "Overview.md": {"content": "# kachery\n\nKachery is a software suite which facilitates sharing data for scientific or technical research.\n\n## What it does\n\nThe kachery software suite facilitates sharing three types of data:\n\n* **Files** are data files, of any size. Files over 20 MiB will be broken into\nsmaller pieces for transfer and caching.\n\n* **Feeds** are *append-only* logs, which operate like a digital\n[lab notebook](https://en.wikipedia.org/wiki/Lab_notebook) or\n[ledger](https://en.wikipedia.org/wiki/Ledger). Because such a feed is append-only\nand signed, it presents a reliable history of actions taken by any agent; and\nbecause it is complete, it offers a single source of truth from which the current\nstate of any system built on kachery can be reconstructed just by replaying the\nlog. For more details, consult the [feeds documentation](./feeds.md).\n\n* **Tasks** are operations acting on files or feeds in the system. The system\nsupports three basic types of tasks:\n  * *Pure calculation tasks* are pure functions in the\n  [computational](https://en.wikipedia.org/wiki/Pure_function)\n  or [mathematical](https://en.wikipedia.org/wiki/Function_(mathematics)) sense.\n  kachery provides a deterministic unique identifier for the combination\n  of a pure function and the arguments it was called with, and maps that identifier to\n  the result of the computation which is cached.\n  This eliminates the need for duplicate calculations,\n  which is particularly useful for computationally intensive processes.\n  * *Queries* are tasks whose result depends on some external state (such as\n  looking up a configuration value in local storage). Because external systems can\n  change, a query task should be rerun on each request, if possible.\n  However, the result\n  of a query task is still loaded into the kachery cache: this allows transfer of data between nodes, as well as providing a fallback result if the external resource\n  that would be queried is not available.\n  * *Actions* are tasks which change the state of some resource, for example appending to a feed. Action tasks do not return any\n  substantive result, only the success or failure of the task. (TODO: do we guarantee each action is only run once?)\n\nFor more details, consult the [tasks documentation](./tasks.md).\n\nThese three types of data, collectively called *information* in these pages,\nare stored in a distributed fashion among the users of the network. The documentation\n[discusses the kachery storage model in detail](./storage.md).\n\nAdditionally, kachery supports permissions that [restrict access to data and compute resources](./security.md).\n\nFinally, nodes may also store local data called *mutables* in a local filesystem database.\nThese are intended to be short-term volatile stores of convenience data (such as channel\nor node aliases) and are not shared between nodes.\n\n## What Can I Do With kachery?\n\nIf you want to send and receive files, you (or your administrator, for shared setups)\nwill need to install and run kachery-daemon on the machine that you intend to be the\nhost of the node. See [full setup instructions for a kachery node](./node-howto.md).\nYou will also need to use kacheryhub to connect your new node to the channel you'd\nlike to use to share data.\n\nOnce this is done, [install kachery-client](./client-howto.md) and you'll be ready to start!\n\nMore sophisticated setups are of course possible: the same tools can be used to build\nweb applications that act like nodes or make use of the [kachery-daemon API](./daemon-api.md).\nThe largest use of kachery is as a library to facilitate other applications, such as\nweb applications; see our [section on building projects that make use of kachery](./building.md).\n"}, "software-components.md": {"content": "# kachery comprises the following software components\n\n* [kacheryhub](https://www.kacheryhub.org/) is a unique, centralized site, but its\n[source code](https://github.com/kacheryhub/kacheryhub) is publicly available.\n\n* [kachery-daemon](https://github.com/kacheryhub/kachery-daemon) is the software that\nbasic nodes run. It defines communication between the components of the kachery network,\nas well as managing the local file store, and access to any defined computational resources.\n\n* [kachery-client](https://github.com/kacheryhub/kachery-client) is a Python client\nfor interacting with a node. (The node may be a separate process running on the same\nmachine, or could be on a shared resource which multiple users might connect to.)\nIt also provides a command-line user interface to request or share files,\nretrieve messages from feeds, etc. See documentation on\n[setting up and using the kachery-client](./client-howto.md)\n\n* kachery-js and kachery-react are typescript libraries for interacting with the kachery network from a web app. \nFor instance, in the [sortingview](https://github.com/magland/sortingview)\ntool, the web server running the labbox\nsoftware takes the place of a command-line client. These projects are not yet separated out into reusable libraries."}, "client-howto.md": {"content": "# kachery-client documentation\n\n**QUERY: HOW IS CONFIGURATION SET? WHAT'S UP WITH THE PROBE STUFF?**\n\n## How to set up a kachery-client instance\n\nSetting up a kachery-client instance is straightforward. We recommend that\nall installation take place in a conda environment or other virtual environment,\nto ensure minimal issues with dependencies and upgrades; please ensure a python\nversion of 3.8 or above.\n\nAfter activating the virtual environment, install [numpy](https://numpy.org/)\nand the `kachery-client` package, both of which are available on\n[pypi](https://pypi.org/)\n\n(SET UP ENVIRONMENT VARIABLES? WHAT'S THE MINIMUM CONFIGURATION REQUIREMENT?)\n\nYou can now access the kachery network via the command-line client.\n\nFull example:\n\n```bash\nconda create --name kachery-client-env python=3.8\nconda activate kachery-client-env\npip install --upgrade numpy kachery-client\n```\n\n### System requirements\n\n[DO WE NEED A KACHERY STORAGE DIRECTORY LOCALLY?]\n\n### Config file\n\n**QUERY: I don't know how this works** need to look into\n**WHERE DOES THE NODE ID COME FROM?** When is it checked against the connected node?\n\n## kachery-client commands\n\nkachery-client can be used over the command line to interact with\nfiles stored in the kachery network. The following commands are\ndefined:\n\n* `cat-file SHA1-URI`: Prompts the attached node to place a copy of the file identified by\n`SHA1-URI` into the kachery storage directory. Once the file is in place, `cat-file` then\nprints its contents to the terminal.\n\n  For instance,\n\n  > `kachery-client cat-file sha1://f728d5bf1118a8c6e2dfee7c99efb0256246d1d3/studysets.json`\n  \n  will (assuming the client is connected to a node which belongs to a channel sharing\n[spikeforest](https://spikeforest.flatironinstitute.org) data) display a JSON file describing the\nset of recordings in that project.\n\n* `load-file SHA1-URI`: As with `cat-file`, this prompts the node to obtain a copy of the file. However,\ninstead of displaying its contents to the screen, it prints the full local filesystem path to\nwhere the file can be found.\n  \n  > `kachery-client load-file sha1://f728d5bf1118a8c6e2dfee7c99efb0256246d1d3/studysets.json`\n  \n  would print some variation on\n`/KACHERY/STORAGE/DIRECTORY/sha1/f7/28/d5/f728d5bf1118a8c6e2dfee7c99efb0256246d1d3`, while\n\n  > ``cat `kachery-client load-file sha1://f728d5bf1118a8c6e2dfee7c99efb0256246d1d3/studysets.json` ``\n\n  (capturing the returned file path with backticks) will send the local filesystem location of the\nstudysets.json file to the `cat` command, thus resulting in the exact same output as the\n`kachery-client cat-file` command above.\n* `store-file LOCAL-FILE`: This causes kachery to make a copy of the file located at `LOCAL-FILE`\nand place it in the kachery storage directory. It then prints the resulting SHA1 URL (derived from\nhashing the file) to the terminal. This SHA1 URL can then be shared with other users of the kachery network\nso that they can request it to be copied to cloud storage for their use.\n* `link-file LOCAL-FILE`: As with `store-file`, this makes a file from the local filesystem accessible\nto the kachery node. However, instead of making a copy, this creates a soft [??] link within the\nkachery storage directory. This avoids having to duplicate the file contents, but comes with the usual\nrestrictions on links: the source file and the link must be on the same file system, and any changes to\nthe original file will cause corresponding changes to the linked version. NOTE: this latter property\nis potentially very problematic for a content-accessible data store, like kachery; if the original\nfile is changed, its hashed contents will no longer match the SHA1 where it is located, making it\nappear invalid. As such, `link-file` should only be used for files that will not be modified further.\n* `generate-node-id`: Generates a new node ID and configures the client to expect to connect\nto a node with this ID. **MORE INFO NEEDED HERE**\n* `version`: Prints the version number, then exits.\n\n## Node vs Client\n\nThe distinction between a *node* and a *client* can\npotentially be unclear, in that many of a node's\noperations are similar to those a client would carry\nout in a client-server architecture. The key distinction\nis that a *client* is an instance of `kachery-client`,\nwhile a *node* is an instance of `kachery-daemon`. Moreover:\n\n* Nodes talk to the rest of the nodes in the network as peers;\neach client only communicates with its main node.\n* Information exchange is conducted through nodes. Clients\nnever directly upload anything to the network.\n* Nodes are members of channels; the client application is\nessentially unaware of the existence of kachery channels.\n* The client cannot communicate, even with a cloud\nstorage cache, without a connection to a running daemon/node.\n(The client offline mode is only for retrieving data from the\nlocal filesystem--and thus for retrieving files; you cannot\naccess feeds or mutables otherwise.)\n\nEssentially, the node is aware of itself as part of a community\nof other nodes; to the client, its one node is the whole world.\n\n## Client-Node Communication\n\nIn order to interact with the kachery network, a client\nneeds to have access to a node. The node can run on the\nsame machine as the client, or the client can be configured\nto connect with a remote node--for instance in a setup where\na lab uses one central machine as a node and the researchers\nhave a client running on their individual workstations or\nlaptops.\n\nFor setups where the client is local to the node, XYZ\nconfiguration is needed.\n\nTo configure a client to communicate with a remote node,\nABC.\n\n### Offline Mode\n\nA kachery client can be placed in \"offline mode\" by setting\nthe environment variable `KACHERY_OFFLINE_STORAGE_DIR` to\na valid path on the client's filesystem. If this environment\nvariable is set, the kachery client will not attempt to\ncontact a node, but will only retrieve files from the\noffline storage directory (which acts as a local cache).\n\nNote that the value of `KACHERY_OFFLINE_STORAGE_DIR` is\nnot checked for validity; if it is set to an invalid\npath (or one which the user running `kachery-client`\ncannot read) then the client will be in offline mode\nbut will not be able to retrieve any files.\n\n## Environment variables governing kachery client\n\n* `KACHERY_STORAGE_DIR`: If set, this defines the directory in\nwhich the client will look for files locally before attempting\nto obtain them from the node. QUERY: The environment variable\nwill be honored over the config file.\n* `KACHERY_DAEMON_PORT`: If set, this variable will define an\nalternate port to use when connecting to the kachery daemon (node).\n* `KACHERY_DAEMON_HOST`: If set, this variable will define the\nhost on which to look for the node. If not set, the daemon\nis assumed to exist on localhost (i.e. the node is assumed\nto be running on the same machine as the client). **CONFIRM**\n* `KACHERY_TEMP_DIR`: If set, temporary files will be stored in\nthis directory.\n* `KACHERY_KEEP_TEMP_FILES`: When this variable is set to 1,\nthe kachery client will not attempt to clear out temporary files.\n* `KACHERY_OFFLINE_STORAGE_DIR`: If set, puts the client in\noffline mode, in which it will not attempt to connect to a\nnode. The value of the variable should be the path to an\noffline storage directory.\n"}, "node.md": {"content": "# kachery Nodes\n\nA *node* is the basic unit of the kachery network: a long-running program that\ncommunicates with other nodes across different *channels* and can potentially\nrequest data or operations from other nodes on the channel or contribute its own.\nThe basic example is a running instance of the `kachery-daemon` package.\n\n*For practical information on running a kachery node, including a step-by-step tutorial,\nsee the [instructions on running a kachery node](./node-howto.md).*\n\n## Communications\n\nThe kachery network is designed as a mediated peer-to-peer information sharing tool\nwith a centralized communication channel structure.\nIt is *peer to peer* in the sense that there is no central repository\nof data which acts as the sole provider of files: any node with permission can\noffer new files to the collective, without requiring centralized approval on\na per-file basis. However, it is *mediated* in that nodes do not communicate directly\nwith each other, and are not required to track how to find other individual nodes;\nfile transfer is accomplished through a cloud storage cache that acts as a set of\norganized cubbies for information drops, while node-to-node coordinating communications\ntake place over a\n[publish-subscribe system](https://en.wikipedia.org/wiki/Publish-subscribe_pattern)\n(currently built on [Ably](https://ably.com/pub-sub-messaging)). TODO: mention how channels play a role in communication.\n\n### Data transfer\n\nNodes in the kachery network do not communicate directly with each other. Instead, nodes\ntransfer information (files or feeds) between one another through cloud storage caches that are provided by kachery channels.\nIf a node requests information that is not already present in the cloud storage cache,\na node on the same channel which has the information can transfer it by copying it into the cache and using\nthe pub-sub system to inform the requester that it is now available in the cache. (Of\ncourse, if the information is already in the cache, the requester can find it without\nany interaction with other nodes whatsoever.)\n\nWrite access to the cloud storage cache is tightly controlled by the channel that owns the cache: in order to write,\na node needs authorization from the channel to write a file with a particular name and size. This\nprocess is described in greater detail, including some limitations, in the\n[security model documentation on uploads](./security.md#Uploads).\n\n### Coordinating communications\n\nThe node-to-node communication channels correspond to the six types of\n[channel permissions](./security.md#Permissions): each node can be granted\npermission on a channel to *request* or *provide* data of the three different types:\n*files*, *feeds*, and *tasks*. Thus, for each named channel on kacheryhub,\nthere are six pub-sub channels:\n\n* Request File\n* Provide File\n* Request Feed\n* Provide Feed\n* Request Task\n* Provide Task\n\nEach of these channels can have both a \"publish\" and a \"subscribe\" permission.\nPublish permission allows the node to write new messages to the pub-sub channel,\nwhile an active subscription causes the node to be notified whenever new\nmessages are published to the pub-sub channel.\n\nGranting a node permissions for a particular action on a named kachery\nchannel automatically grants permissions on the corresponding pub-sub\nchannel that will allow it to fulfill that role. These permissions\nhave a complementary distribution: to request a type of information,\na node needs to be able to publish to the request channel and be subscribed\nto updates from the provide channel; by contrast, to provide information,\na node must be notified of new messages on the request channel, and able\nto publish updates to the \"providing files\" channel.\n\nThe following table illustrates the pub-sub channel permissions corresponding\nto the \"Request File\" and \"Provide File\" permissions:\n\n| Permission   | Request Files channel | Provide Files channel |\n|--------------|-----------------------|-----------------------|\n| Request File | Publish allowed       | Subscribed            |\n| Provide File | Subscribed            | Publish allowed       |\n\nPermissions to request or provide feeds or tasks are similar.\n\nWhen a node *requests* information, it publishes the request to the\ncorresponding \"Request\" pub-sub channel. All nodes in the kachery channel with the corresponding\n*provide* permission for that information are then notified of the new\nrequest by their subscription. A node which can satisfy the request\nwill publish a response to the \"Provide\" pub-sub channel; the requesting node\nsees this notification and knows that its request will be satisfied.\nThe provider node continues to publish updates during the retrieval\nprocess, and will publish a \"completion\" notification after all data\nis available in the cloud storage cache. The requesting node is\nnotified by subscription that the data transfer to the cache is complete,\nand then begins downloading the requested data from the cloud storage cache.\n\nFeeds are somewhat unique, because feeds are owned by a particular node:\nnodes that do not own the feed will not upload feed data to the cloud\nstorage, even if they possess feed data which is more recent than what is\nin the cloud cache. This may be changed in a future version.\n"}, "content-uris.md": {"content": "# Content URIs\n\nContent URIs are at the heart of the kachery system. Files in kachery are never referred to by local file path (e.g., `/path/to/file.dat`) or by web URL (e.g., `http://example.com/file.dat`), but are instead referenced by content URIs of the form\n\n```\nsha1://943a702d06f34599aee1f8da8ef9f7296031d699\n```\n\nThe above 40-character string is the SHA-1 hex digest for the string `\"Hello, world!\"`. If we are running a kachery node that belongs to a channel containing this string, then we can retrieve this text from the command-line:\n\n```bash\nkachery-cat sha1://943a702d06f34599aee1f8da8ef9f7296031d699\n# Output: Hello, world!\n```\n\nor from Python:\n\n```python\nimport kachery_client as kc\n\ntxt = kc.load_text('sha1://943a702d06f34599aee1f8da8ef9f7296031d699')\nprint(txt)\n# Output: Hello, world!\n```\n\nIf the kachery channels that our node belongs to do not contain this string, we can add it via:\n\n```python\nuri = kc.store_text('Hello, world!')\nprint(uri)\n# Output: sha1://943a702d06f34599aee1f8da8ef9f7296031d699\n```\n\nThis small string is not very interesting, but the advantages of the content URI scheme start to become evident when we consider larger files. For example, the following is a reference to the plain-text e-book version of *Treasure Island* from Project Gutenberg:\n\n```\nsha1://ab42927dabd81c0d7993e369a6a2a1551305aaac/treasure-island.txt\n```\n\n(The `treasure-island.txt` part of the string is just for information and does not affect the represented content.)\n\nIf we want to count the number of times the word \"captain\" occurs in this e-book, we can use the following universal script:\n\n```python\nimport kachery_client as kc\n\ndef count_captain(text_uri: str):\n    txt = kc.load_text(text_uri)\n    word = 'captain'\n    return txt.lower().split().count(word)\n\nct = count_captain('sha1://ab42927dabd81c0d7993e369a6a2a1551305aaac/treasure-island.txt')\n\nprint(f'The word \"captain\" occurs {ct} times in the e-book version of Treasure Island.')\n\n# Output: The word \"captain\" occurs 102 times in the e-book version of Treasure Island.\n```\n\nNow we have a portable script that will always output the same correct result no matter where it is run, assuming that we have access to the file referred to by this URI. In this way we have separated the issue of locating data from the process of specifying a well-defined processing pipeline.\n\n## Pure calculations\n\nA further advantage of this content URI scheme is that it facilitates caching and sharing of *pure calculations*, or calculations that depend only on input arguments and do not read or write to any external state. In kachery we can define a pure calculation function as follows:\n\n```python\nimport kachery_client as kc\n\n@kc.taskfunction('count_captain.1', type='pure-calculation')\ndef count_captain(text_uri: str):\n    txt = kc.load_text(text_uri)\n    word = 'captain'\n    return txt.lower().split().count(word)\n```\n\nRead more about tasks and pure calculations [here](./tasks.md)."}, "node-howto.md": {"content": "# How to Run and Configure a kachery Node\n\nMost of the action in the kachery network involves communication between\n[nodes](./node.md). These are the network peers that exchange information\nand maintain offline storage in the content-addressable database; nodes\nmay also optionally be the owners of [feeds](./feeds.md) or provide\n[task execution resources](tasks.md).\n\nTo get started working with the kachery network, simply set up your\nown node, register it with [kacheryhub](./hub.md), and join a [channel](./channel.md). Once the\nchannel owner has granted your node access permissions, you will be able\nto exchange information with other nodes on the channel.\n\n## Running a Node\n\n*For the most up-to-date information on creating and managing a node, see\nthe [kacheryhub website](https://www.kacheryhub.org/home).*\n\nA kachery node can be hosted on anything from a modest user machine\n(such as a laptop) to a large shared lab resource with abundant\nnetwork-attached storage. A very straightforward model is to run\na node on your individual workstation or laptop. To do this, you will\nneed a POSIX-compliant environment capable of running Python >= 3.8. If you want to join one or more kachery channels, you will also need access to a network connection and a free Google account for\nauthentication with kacheryhub.\n\nWe highly recommend using Conda or another virtual-environment provider\nwhen setting up your node, for ease of updates and to avoid potential\nversion conflict issues.\n\nThe basic setup looks like this:\n\n```bash\nconda create --name kachery-node-env python=3.8\nconda activate kachery-node-env\n# Linux version:\nconda install -c conda-forge nodejs\n# MacOS version, if the linux version does not give you nodejs >= 12:\n# conda install nodejs -c conda-forge --repodata-fn=repodata.json\npip install --upgrade numpy kachery-daemon\n# You may also choose to pip install --upgrade kachery-client, if you intend\n# to use the same virtual environment for all kachery interactions.\nkachery-daemon-start --label <WHATEVER YOU CHOOSE> --owner <YOUR GOOGLE ACCOUNT ID>\n# The label flag can have any value: it will be visible on kacheryhub, so it should be\n# appropriate for public use, but it need not be unique.\n# The owner flag should be passed a full Google account address, e.g.:\n# --owner sergey.brin@google.com\n```\n\nLeave this running, and in another terminal window, use:\n> `kachery-daemon-info`\nto print the node ID of the running node.\n\nYou may find it convenient to use [tmux](https://github.com/tmux/tmux/wiki)\nfor terminal session management, instead of opening additional terminal\nwindows.\n\nLog in to kacheryhub with the same google account you passed for the\n`--owner` flag when starting\nthe daemon, then click the `add kachery node` button and follow the\ninstructions on screen to register your node.\n\nYou can now click on your node ID to join channels. Indicate the roles\nyou would like to perform, and inform the channel owner (through another\nmeans of communication) of your name and node id. Once you've been authorized,\nyou can start using the client commands discussed in the\n[kachery-client documentation](./client-howto.md).\n\n## Advanced Configuration\n\nTODO\n\n**QUERY:** ARE THERE COMMAND-LINE OPTIONS FOR CONFIGURING PORTS ETC.\nTHAT WE SHOULD DISCUSS HERE?\n\n### Configuring a remote node\n\nTODO\n\n### Configuration for task providers\n\nTODO\n\n## Environment Variables for kachery Nodes\n\n* `KACHERY_STORAGE_DIR`: should be set to a directory where you would\nlike kachery to store files locally. This may require considerable space,\ndepending on how much data you intend to transfer. The user running the\n`kachery-daemon` process will need read and write access to this directory.\n\n  If it is unset, this directory will default to `$HOME/kachery-storage`.\n\n* `KACHERY_DAEMON_PORT`: sets the port on which your instance of the\n`kachery-daemon` process will listen for incoming connections from clients.\nNote that this is not related to the communication with the channel or broader\nkachery network; those communications happen via outbound requests on standard\nHTTPS ports. This variable is only used to set the port over which you will\nallow instances of the `kachery-client` application to communicate with your\nnode, in the event you are providing services as a remote node.\n\nTODO: document other env variables"}, "web-apps.md": {"content": ""}, "tasks.md": {"content": "# Tasks\n\nThe main function of tasks in kachery is to provide a caching mechanism for expensive\ncomputations--the \"pure-calculation\" task type. However, the task mechanism also provides\na convenient hook to allow kachery nodes or compute resources to execute python\ncode written by researchers. Thus two additional types of task are supported:\n\"query\" tasks which depend on state that is not present in their arguments, and\nthe \"action\" tasks which modify state outside the kachery system.\n\n## Example\n\nExposition below will assume the following code (`backend.py`) is being run on a\nmachine that is meant to execute kachery tasks.\n\n```python\n#!/usr/bin/env python3\n\nimport kachery_client as kc\nimport hither2 as hi\n\n@kc.taskfunction('echo_immediate.2', type='pure-calculation')\ndef echo_immediate(*, a: str) -> str:\n    return f'a is {a}'\n\n@hi.function('echo_hither_fn', '1')\ndef echo_hither_fn(*, b: str) -> str:\n    return f'b is {b}'\n\n@kc.taskfunction('echo_hither_task.1', type='pure-calculation')\ndef echo_hither_task(*, b: str) -> hi.Job:\n    job = hi.Job(echo_hither_fn, {'b': b})\n    return job\n\ndef main():\n    kc.run_task_backend(\n        channels=['my_channel'],\n        task_function_ids=[\n            'echo_immediate.2',\n            'echo_hither_task.1'\n        ]\n    )\n\nif __name__ == '__main__':\n    main()\n```\n\nSuppose an instance of `backend.py` received a request for the task `echo_immediate.2`\nwith parameter `{'a': \"hello world\"}`. The `run_task_backend` function looks up\nthe `echo_immediate.2` ID and executes the corresponding python function `echo_immediate`,\nreturning the value `hello world`. This result is then cached; so if any\nfuture node wants to request the value of function ID `echo_immediate.2` with\nparameter \"hello world\", the *requesting node* would check the cache first, find\nthe result, and be able to sidestep the entire task request.\nHowever, a request for `echo_immediate.2` with `{'a': 'goodbye}'` would\nnot be in cache, and would require `echo_immediate` to be run again.\n\nThe above example also demonstrates the use of a hither function for asynchronous\nexecution. A request for `echo_hither_task.1`\nwith the value `{'b': 'hello world'}` causes the `echo_hither_task` python\nfunction to be called. That task returns a hither Job, which is an asynchronous\ndeferred processing token that may be executed in a container and/or on a remote\nresource or cluster node.\nWhile the Job is processing, the task backend started by `backend.py` will\npublish status updates on the `PROVIDE-TASK` pub-sub channel. Once\nprocessing is complete, the return value of the Job will be placed in the kachery cache,\nso nodes subsequently requesting `echo_hither_task.1` with the same `{'b': 'hello world'}`\nparameter can retrieve the reslt from the cache without invoking the task system or\nre-running either `echo_hither_task` or the `echo_hither_fn` hither function it wraps.\n(Note that hither itself can also be configured with an independent local job cache,\nso that even if the result of a particular kachery task is cycled out of the\nkachery cache, re-running the hither Job can still avoid repeating a long-running\ncomputation.)\n\n## Task Execution\n\nTasks are executed by a *task backend*. The task backend is a persistent service\nprovided by the `kachery-client` package, and is a distinct process from the\ndaemon node provided by `kachery-daemon` (although they will usually be run on the\nsame machine). A task backend is initiated when a user-defined script makes a\ncall to `kc.run_task_backend` and will terminate when that script is terminated.\n\n### Task Backend-to-Node Communication\n\nIn the example above, the task backend is initiated by running `backend.py`.\nThis process must have a connection [DESCRIBE HOW] to a node that is on the channel\n`my_channel`, as declared in the `run_task_backend` call. The task backend\nconnects to the node by making a long-running HTTP [VERB ?] request with a list of the\nregistered function IDs; thus the node immediately knows what tasks are available,\nand can request any of them to be run by responding to the HTTP request with the\ndesired task ID and parameters. When the task backend receives a request to execute\na task, it simply calls the corresponding python function.\n\nThus, the task backend communicates only with one node; most of the distributed\ninteractions take place at the level of node-to-node communication.\n\n### Node-to-Node Communication\n\nEach node with active \"Provide Task\" permissions on a channel must be connected\nto a task backend in order to run tasks successfully. The \"Provide Task\" permission\ngrants \"publish\" (write) access to the `PROVIDE-TASK` pub-sub channel, and \"subscribe\"\n(read) access to the `REQUEST-TASK` channel (see the reference on\n[node-to-node communications](./node.md#communications)).\n\nWhen a node (\"**Requester**\") wishes to run a task, the following occurs:\n\n* **Requester** generates a deterministic unique ID **ID** based on the task ID and the\narguments to be passed.\n* **Requester** checks the cloud storage cache for an entry matching this **ID**. If\nfound, **Requester** uses this cached value, and no further action is needed. (This system\nwill function even if there are no active nodes on the channel with \"Provide Task\" permissions.)\n* If the value is not cached:\n  * **Requester** publishes a request with the task ID and parameters on the `REQUEST-TASK`\n  pub-sub channel for the kachery channel.\n  * Every node (\"**Provider**\") with active \"Provide Task\" permissions on the kachery channel\n  is subscribed to the `REQUEST-TASK` pub-sub channel, and thus notified of the request.\n  * Each **Provider** checks whether its task backend provides the requested task.\n  * **Provider** nodes that don't recognize the task do nothing.\n  [ED: SHOULD THEY PUBLISH SOMETHING EXPLICIT TO `PROVIDE-TASK`?]\n  * Each **Provider** node with a matching registered task ID instructs its task backend\n  to call the corresponding python function (and provides it the parameter list\n  from **Requester**). Each **Provider** publishes its status on the `PROVIDE-TASK` channel,\n  so that **Requester** can fetch the results from the cloud cache once they are available.\n  * For tasks implemented as hither functions, the **Provider** will publish Job status\n  updates to the `PROVIDE-TASK` pub-sub channel, as well as the final results.\n\nThe exact contents of any particular task is up to the task implementer. However,\nexecuting a task is a blocking call, so for anything\nlong-running, it is advisable to use [hither](https://github.com/flatironinstitute/hither)\nor another means of executing the task asynchronously, to prevent the task backend from\nhanging for an unacceptable length of time.\n\nThis description applies to \"pure-calculation\"-type tasks. The other two task\ntypes are handled slightly differently:\n\n* In the case of \"query\"-type tasks, **Requester** will make the request on the\n`REQUEST-TASK` channel first, and only use cached results if new ones cannot be obtained.\n* In the case of \"action\"-type tasks, the cache is skipped entirely, as these tasks are\nintended to modify external state. [QUERY: DO WE NEED A \"RUN-ONCE-ONLY\" FEATURE?]\n\n## Registering Tasks\n\n*Tasks* are a mechanism to run user-defined python code. However, the system does not allow\narbitrary code execution: a task backend will only run tasks which the implementer has\nexplicitly registered through the `task_function_ids` parameter of `run_task_backend`.\n\nTo be registered, a python function must also be decorated with the `@taskfunction` decorator.\nThis decorator takes a unique identifier for the task function, as well as a type (one\nof \"pure-calculation\", \"query\", or \"action\".) Registered functions should take only `kwargs`\nparameters. Moreover, they can only accept parameters that are JSON-serializable: either\nbasic types, or a few others with explicit serialization support (e.g. Numpy arrays). In no event\nshould a task function be passed executable code: parameters come from an arbitrary node on the\nnetwork, and (permissions/authorization notwithstanding) they should be considered untrusted\ninputs that need to be sanitized before being used on secured systems.\n\n## Task Permissions\n\nThe \"Request Task\" and \"Provide Task\" permissions govern task access on a channel. Nodes can\nrequest task results if granted \"Request Task\" access, which will also allow *publish* rights\non the channel's `REQUEST-TASK` pub-sub channel and *subscribe* rights on the channel's\n`PROVIDE-TASK` pub-sub channel. Nodes can provide task results if granted \"Provide Task\" access,\nwhich provides complementary pub-sub rights (*subscribe* on `REQUEST-TASK` and *publish* on\n`PROVIDE-TASK`).\n\nBecause of this structure, task requests are inherently associated with a specific channel.\nHowever, in the event that multiple channels share a single cloud storage cache, a node could\nconceivably retrieve task results from a different channel; so it is advisable not to share\ncloud storage caches between channels.\n\nPermission is currently granular at the node level, not the task level. A node that can\nrequest task execution is allowed to request any task registered with the backend.\n\n## Task Results\n\nResults of \"pure-calculation\" or \"query\" tasks will be written to the cloud storage cache.\n\"Action\"-type tasks only return success or failure, which will be communicated over the\npub-sub channel.\n\nIn the event that the python function backing a task request returns a hither Job, the\ntask backend will automatically write the Job result to the cloud cache on Job completion.\nIf the hither Job fails, [WHAT HAPPENS?]\n\n## Caveats\n\n### Channel-Storage Interactions\n\n[IS THIS TRUE?]\nAny task request must specify which channel it wants the response from. However, a node\nlooking for task results will potentially look at every cloud storage it is aware of,\nnot only the storage associated with the channel on which the request was made. While it\nis extremely unlikely that two sets of unrelated job results would have the same fingerprint,\nin specific circumstances it might be possible for a node to obtain or share results from a\ndifferent channel where it might not have the same permissions.\n\n### Race conditions for task execution\n\nIn the current design, task provider nodes do not interact with each other. This could\npotentially lead to multiple nodes attempting to execute the same task simultaneously\non different backends, which is potentially inefficient for computationally intensive\ntasks (and could be problematic for action-type tasks).\n\nThere can also be an issue with multiple provider nodes uploading data to the same\nfile location on the cloud bucket, if multiple providers' instances of the task finish\nat different rates.\n"}, "pub-sub.md": {"content": ""}, "storage-bucket.md": {"content": ""}, "hub.md": {"content": "# kacheryhub\n\nPeer-to-peer networks experience a tension between several competing goals.\nOn the one hand, decentralization and ease of growing a network are useful\nproperties that promote openness and resiliency;\nhowever, they also present challenges to peer\ndiscovery, inter-node communication, and controlling permissions/access.\n\nThe current kachery design strikes a balance by choosing a\n[mediated peer-to-peer structure](./node.md#communications). In this design,\nthere is no permanent central repository of data: this removes the administrative\nburden and some of the expense of running a central data server, as well as\nproviding resilience by distributing copies of important data to many different\nmachines. At the same time, the design's central coordinating hub and\npublish-subscribe inter-node communication model alleviate several problems of\na purely distributed system. By providing a central location for nodes to\nregister with channels, and by ensuring each channel has a central point\nof contact, all nodes can communicate using outbound, node-initiated\nsignals. This alleviates the challenges posed by firewalls in direct\npeer-to-peer setups.\n\n*kacheryhub* serves as a central web site for channel management and node\nregistration. A user can create a channel and publicize it to potential\nmembers (see [documentation on channels](./channel.md) for requirements), and\nanyone running a node can register that node and join one or more channels.\nChannel owners also\n[use kacheryhub to manage the permissions](./security.md#permissions)\ngranted to nodes that connect to the channel.\n\n## Functions\n\n* **Authentication/Authorization**: users sign in to kacheryhub with an account\nfrom a federated authentication provider (e.g. Google). The kacheryhub site also\nassociates the account with owned nodes and channels.\n\n* **Node registration**: a user running a [node](./node.md)\nmust register it with kacheryhub\nso that channel owners can give it permissions. Nodes are registered and unique\naccording to the combination of node ID (automatically generated by the node)\nand owner (Google account). This prevents a user from impersonating another\nuser by registering a node under their name; to do so, the attacker\nwould have to be able to log in to kacheryhub with the\nother user's Google account credentials.\nTODO: THIS IS VAGUE, WHAT IS THE ATTACK VECTOR WE'RE PROTECTING FROM HERE?\n\n* **Node channel membership**: a node's owner can use kacheryhub to register\nthe node with one or more channels, allowing it to share information with the\nother nodes on the channel. As a channel owner, you can limit what actions your\nnode will engage in on the channel: for instance, if you would prefer that your\nnode not provide files on the channel, your node will not do so, even if the\nchannel owner has granted you that permission. Node owners can also control\nwhether the node will download information from the channel. (WHAT'S THE PURPOSE\nOF THIS?)\n\n* **Channel management**: [channels](./channel.md) are communities of nodes\nthat share information. Any kacheryhub user can create a channel by providing\na *unique name*, access credentials to a\n*cloud storage resource* that will serve as the channel's cloud storage cache, and\na *publish-subscribe API key* (i.e. [Ably](https://ably.com/pub-sub-messaging))\nto authorize nodes to communicate on the channel's pub-sub bands. These credentials\nwill be stored on the kacheryhub system and used to authorize access from\nother nodes to the pub-sub communications channels and the cloud storage cache.\nThese credentials must be stored in a readable format in order to be used in\nthis fashion; however, they are revocable in the unlikely event of compromise.\n\n## Requirements\n\nTo interact with the systems on kacheryhub, you will need at least an account\nwith a federated authentication/authorization provider (i.e. Google), as well as\na computer system under your control where you can host a node. (While it is\npossible to interact with kacheryhub without this, there is little meaningful\naction available if you don't have a node.) It is not technically required to\nhave a node in order to register a channel, but for most use cases, the channel\nowner would want to have a node with which to begin providing data--it would be\nrather unusual for a channel to be managed by someone who does not participate.\n"}, "sharing-data.md": {"content": "# Sharing data between workstations using Python\n\nThe easiest way to share data using kachery is via the [kachery-client](https://github.com/kacheryhub/kachery-client) Python package. After starting a [kachery daemon](https://github.com/kacheryhub/kachery-daemon) on your local computer, you can store data to the kachery network as in the following examples.\n\n## Static content (files)\n\n```python\nimport kachery_client as kc\n\n# You need to be running a kachery daemon.\n# If you want to share these data with remote\n# computers, then your kachery node must be\n# configured to provide files on some channel,\n# and the remote computers must be configured\n# to request files on that channel.\n\n# Store some text\nuri = kc.store_text('some-random-text')\nprint(uri)\n# Output: sha1://6af826b3d648ccba6b4bbe58e93e22add640d728/file.txt\n\n# Later on retrieve the text by its content URI\ntxt = kc.load_text('sha1://6af826b3d648ccba6b4bbe58e93e22add640d728/file.txt')\nprint(txt)\n# Output: some-random-text\n\n# Similarly, you can store/share json-able Python dicts,\n# numpy arrays, or pickle-able Python objects\nimport numpy as np\nuri_dict = kc.store_json({'a': [1, 2, {'b': 3}]})\nuri_npy = kc.store_npy(np.ones((10, 20)))\nuri_pkl = kc.store_pkl({'x': np.zeros((30, 40))})\n\n# And retrieve then at a later time\nX_dict = kc.load_json(uri_dict)\nX_npy = kc.load_npy(uri_npy)\nX_pkl = kc.load_pkl(uri_pkl)\n```\n\nThose URIs can also be used to retrieve the data on a remote computer, provided that the remote machine is also running a kachery daemon, the local and remote nodes belong to a [common channel](./channel.md), and the local and remote nodes are configured on that channel to provide and request files, respectively.\n\n```python\nimport kachery_client as kc\n\n# Run this on a remote machine.\n\n# You need to be running a kachery daemon,\n# and your kachery node must be configured\n# to request files on the channel containing\n# the file.\n\ntxt = kc.load_text('sha1://6af826b3d648ccba6b4bbe58e93e22add640d728/file.txt')\nprint(txt)\n# Output: some-random-text\n```\n\n## Feeds\n\n```python\nfrom typing import List, Set\nimport kachery_client as kc\n\n# You need to be running a kachery daemon.\n# If you want to share these data with remote\n# computers, then your kachery node must be\n# configured to provide feeds on some channel,\n# and the remote computers must be configured\n# to request feeds on that channel.\n\ndef prime_sieve(n: int):\n    is_composite: Set[int] = set()\n    primes: List[int] = []\n    for j in range(2, n + 1):\n        if not j in is_composite:\n            primes.append(j)\n            for k in range(j * j, n + 1, j):\n                is_composite.add(k)\n    return primes\n\nfeed = kc.create_feed()\nsubfeed = feed.load_subfeed('primes')\n\nN = 100\nprimes = prime_sieve(N)\nfor i, p in enumerate(primes):\n    subfeed.append_message({\n        'p': p,\n        'n': i + 1\n    })\n\nprint(f'Found {len(primes)} prime numbers less than {N}')\nprint(subfeed.uri)\n\n# Output:\n# Found 25 prime numbers less than 100\n# feed://c21cfca1b54ba841a7cbf35685b5a62db73ed16b7c3f50f2fa1f9e1fce9b9cef/primes\n```\n\nLater on, retrieve the messages either on the same node, or on a different node that has access:\n\n```python\nimport kachery_client as kc\n\n# You need to be running a kachery daemon,\n# and your kachery node must be configured\n# to request feeds on the channel containing\n# the feed.\n\nsubfeed = kc.load_subfeed('feed://c21cfca1b54ba841a7cbf35685b5a62db73ed16b7c3f50f2fa1f9e1fce9b9cef/primes')\nmessages = subfeed.get_next_messages()\nfor msg in messages:\n    print(f'Prime {msg[\"n\"]}: {msg[\"p\"]}')\n\n# Output:\n# Prime 1: 2\n# Prime 2: 3\n# Prime 3: 5\n# ...\n# Prime 25: 97\n```\n\nSee [feeds](./feeds.md) for more information.\n\n## Tasks\n\nIt is also possible to share processing results, or the outputs of pure calculation results, between nodes, and to request that tasks be run on remote nodes.\n\nSee [tasks](./tasks.md) for more information."}, "building.md": {"content": "# Building an Application That Uses kachery\n\n## Requirements\n\nTODO What does an app using kachery need to provide to satisfy the interface?\n\n## Package Functionality\n\nTODO What tools does the existing kachery code base provide?\n\n## Using Nodes via API\n\nTODO, also xref the daemon-api page\n\nTODO something about system requirements doing it this way (i.e. need a\nrunning kachery node)\n\n## Providing a Lightweight Node\n\nTODO, is this even a thing right now?\n\n## Examples\n\n* sortingview (describe project & how used)\n\n* the mcmc-monitor project (describe project & how used)\n\n* ...etc?\n"}, "daemon-api.md": {"content": "# kachery-daemon api\n\nThe following verbs are defined by the kachery daemon api:\n\n## Files\n\nThese verbs describe the interface to add or retrieve files from the\nfile data store. The daemon will store files on a filesystem to\nwhich it has access (as referred to by the `$KACHERY_STORAGE_DIR`\nenvironment variable). Loading a file into kachery means making it\navailable in the kachery-storage filesystem, while retrieving a file\nfrom other nodes entails copying it from the shared cloud\nstorage bucket into the kachery-storage filesystem.\n\n### probe\n\n### stats\n\n### storeFile\n\n### linkFile\n\n### downloadFileData\n\n### store\n\n### loadFile\n\n## Feeds\n\n### feed/createFeed\n\n### feed/deleteFeed\n\n### feed/getFeedId\n\n### feed/appendMessages\n\n### feed/getNumLocalMessages\n\n### feed/getFeedInfo\n\n### feed/watchForNewMessages\n\n## Mutables\n\n*Mutables* represent a key-value store within this particular instance\nof the daemon. They are not shared with other nodes [IS THAT TRUE?] and\nare expected to contain a small amount of ephemeral data. Examples\nwould be a human-readable alias for channels or feeds; the alias is\nuseful to the human interacting with the node, but need not be globally\nunique, because it isn't shared with the entire kachery network.\n\n### mutable/get\n\n### mutable/set\n\n## Tasks\n\n*Tasks* are user-defined python code which has been allowed to be\nexecuted on kachery node compute resources. The basic use case is\nto cache the results of expensive pure-function computations, but\nthe same mechanism provides hooks for applications which either\ndepend on or alter external state. See [tasks](./tasks.md) for more\ndetails.\n\n### task/registerTaskFunctions\n\n### task/updateTaskStatus\n\n### task/createSignedTaskResultUploadUrl\n\n### task/requestTask\n\n### task/waitForTaskResult\n"}, "channel.md": {"content": "# kachery channels\n\nWithin the kachery network, a *channel* is both a community of\nnodes which may share information and communicate indirectly with\neach other (subject to their granted [permissions](./security.md#Permissions)),\nand the infrastructure through which that information sharing and communication\noccurs. The kachery network is designed as a *mediated peer-to-peer* information\ntransfer network: nodes are all potential peers which can both contribute and\nreceive information, but they communicate with each other indirectly, using the\nresources provided by the channel.\n\n## Channel Resources\n\nIn order to create a channel, two technical resources are needed:\n\n* A *cloud storage space* such as a Google Cloud Storage Bucket; and\n* A *publish-subscribe service*.\n\nIn addition, each channel needs to have a unique name. Channels can be registered and configured using [kacheryhub.org](https://kacheryhub.org) and are owned by users that authenticate using Google sign in.\n\nThe cloud storage space provides a central location to which all nodes on the channel\ncan make HTTP requests. Outbound requests avoid most firewall issues, and also ensure\nthat individual nodes do not need to keep track of how to contact a large swarm\nof other nodes. To minimize the expense of hosting large files permanently on\nthe cloud, this storage space is conceived of as a cache\u2014it need not store most\nof the information on the channel at any one time, and information on the storage\ncache can be cleared out when it has not been used in a while. While a central\nstorage cache could present a single point of failure, choosing a reputable\ncloud storage provider greatly minimizes this risk\u2014it is far more likely that\nan individual node (which might be the only one hosting a file) would go down\nthan that Google would have an extended outage. Moreover, thanks to the central\ninformation transfer channel, nodes can continue to retrieve information that\nremains in cache, even if no active node is available to provide that data.\n\nFor details on the publish-subscribe communication model, see the documentation on\n[node-node communications](./node.md#Communications). Note that the subscription\nwith the pub-sub provider will need to be provisioned adequately for the traffic\nexpected on the channel: a small channel with few peer nodes is unlikely to need\nmore than a basic account, while users using kachery to host a large-scale public web app may require\nmore requests per minute.\n\n## Channel Permissions\n\nChannels, along with their membership and permissions, are managed via the\n[kacheryhub](https://www.kacheryhub.org/home) website. You can\n[read more about kacheryhub here](./hub.md).\n\n## Managing a Channel\n\nFor the most up-to-date instructions on creating and managing a channel, see the\n[instructions on kacheryhub](https://www.kacheryhub.org/home).\n"}, "security.md": {"content": "# Security Model for kachery\n\nThe primary objective of kachery is to make scientific data available. Therefore, while there are mechanisms for restricting access to kachery data, the security model for kachery is generally aimed at preventing abuse rather than keeping data private. Nevertheless, nobody can ever access any data on the kachery network unless they have access to the appropriate URI strings (content hashes or feed IDs). So the most reliable way to keep data files private is to simply keep the content hashes private. Conversely, making data public is largely a matter of making content URIs and feed IDs public.\n\n## Permissions\n\nPermissions are assigned through kacheryhub, on a per-channel\nbasis. The owner of each named kachery channel assigns permissions to\nindividual [nodes](./node.md). The possible permissions are:\n\n* Request Files\n* Provide Files\n* Request Feeds\n* Provide Feeds\n* Request Tasks\n* Provide Tasks\n\nA node which lacks \"request\" permissions for a given information\ntype *cannot* solicit other nodes to provide missing data; it is\nrestricted to only the data that is already present in the\ncloud storage cache. (Note that this information may change from\ntime to time due to files being evicted from cache.)\n\nA node without \"provide\" permissions for a given information type\n*cannot* upload data into the cloud storage cache, and cannot even\nsee requests coming from other nodes.\n\nEach permission on a named kachery channel implies a certain set of\naccess permissions to the pub-sub communication channels. See\n[the documentation on node-node communications](./node.md#coordinating_communications)\nfor details.\n\nPermissions cannot be forced on a node unilaterally. The person owning\nor running a node must specify what permissions the node requests on\neach channel to which it is joined. Ultimately, the actual permissions\nwill be the intersection of the permission lists: nodes have only\nthose permissions which have been both requested by the node owner\nand granted by the channel owner.\n\nNote that permissions are granted to a combination of the *node*\n(identified by a node key) and its *owner* (identified by the federated\nauthentication acccount provider, e.g. a Google account). Thus, even in a\nsituation where multiple users share logins to the same machine and run\na node with the same key, permissions are specific to the user named by\nthe `--owner` parameter with which the kachery daemon was invoked.\n\n## Uploads\n\nThe cloud storage cache is a (paid) resource which should be owned by the\nchannel owner. However, to allow semi-decentralized data sharing among peers,\nthe kachery network must permit other nodes--which are likely owned by\nother parties--to write to cloud storage. This is accomplished by means of\n*[signed upload URLs](https://cloud.google.com/storage/docs/access-control/signed-urls)*\nwhich act something like a prepaid mailing label for files. When a node wishes to\nupload a file to the cloud storage cache, it requests a pre-signed URL\nfrom kacheryhub. This is a URL for a specified file name and size which\nallows upload to a specific cloud storage location. An entity that knows this\nURL can make an HTTP PUT request to it in order to upload a file to the cloud.\n\n## Channel owner stored credentials\n\nkacheryhub manages access to the various resources owned by the channel\nowner: specifically, the cloud storage cache and the Ably pub-sub channels.\nIn order to provide nodes with access, kacheryhub needs to store\nthese credentials and provide limited-access tokens upon\nappropriately validated request.\n\nIn order to create upload URLs, kacheryhub needs to use the\n*channel owner's cloud storage provider access credentials*. These should\nhave been registered at the time the channel was created.\n\nkacheryhub also requires the Ably API key to provide pub-sub channel\naccess tokens to nodes. To interact with other nodes, a node requests\nthe appropriate subscribe and publish permissions from kacheryhub.\nThe kacheryhub server acts as an authentication/authorization server;\nit verifies the identify of the node-owner pair (using signature verification)\nand, if the node checks out, it uses the Ably API key of the channel owner\nto provide the node with temporary access tokens to the pub-sub channels.\n(These expire after around 30 minutes and must be renewed at that time. This is managed internally by the kachery libraries.)\n\n## Feeds\n\nBecause kachery file storage is content-addressable, files are essentially\nself-verifying: the file name is the cryptographic hash, which can\nbe readily checked by anyone who obtains the file.\n\nFeeds, however, will grow over their lifetimes, and so require a different\nmechanism for verification. This is accomplished by public-key cryptography.\nEach feed ID is a public key, whose private counterpart is kept by the node\nwhich owns the feed. Each message appended to the feed must be signed\nby the feed owner's private key. This signature is checked by any\nrequesting node when the feed's data is downloaded.\n\nHaving a unique owner node also makes feeds different from other\ntypes of information shared over the kachery network. Any node that\nhas a file can legitimately upload it, regardless of who\ncreated it; but only the owner node can provide a\ncanonical version of a feed. While the owning node may accept\nnew messages into a subfeed that come from other nodes, those\nnew feed entries have to be approved by being signed by the\nowning node.\n\n## Caveats\n\n* Permissions are assigned on a per-node basis, not a per-file basis.\nThe assumption is that all files (/feeds/tasks) shared on a channel\nare of the same sensitivity and should have the same access rights. Nevertheless, nobody can access files without knowing the corresponding URI strings. Therefore, access can be managed to some extend by restricting access to these URIs.\n\n* In particular, there is no distinction in the\npermission system between a node being allowed to upload files that were\noriginally provided by other nodes (and perhaps have fallen out of the\ncloud cache), and that node being allowed to provide new files for which\nit is the original source.\n\n* As described in the documentation for running nodes, it is possible\nto configure a node so that remote clients can connect to it. (This allows,\nfor instance, a lab's members to use a single central node to talk with the\nwider network, accessing the shared node through clients on their individual\nworkstations.) This greatly simplifies the administrative burden for both\nchannel owners and individual labs, but it does mean that any individual\nwho can connect to a node will have the full set of permissions granted to that node.\nUltimately, any system must contend with this in some fashion--any person\nwith access to privileged files could potentially choose to share them with\nothers--but to avoid accidents, it is important that the owners of trusted nodes\nare informed and responsible parties who can be expected to allow access to\ntheir nodes appropriately.\n\n* Files are not flagged with the channel they came from. So, if channel A\nis configured to share its cloud storage cache with channel B,\nthen a node with access to channel A could conceivably retrieve files associated\nwith channel B in certain limited circumstances. To avoid this issue, channel\nowners should be careful about sharing one cloud storage space between multiple channels.\n\n* The pre-signed URL for file upload includes the name of the storage object and size of the file to\nbe uploaded. Since kachery is a content-addressable\nstorage system, the name of the storage object will contain the hash of its contents.\nFiles can be verified by confirming that the file hashes out to the value indicated\nby its name.\nHowever, because the cloud storage cache has no attached processing, there is no\nmechanism for verifying this property at upload time in the cache itself: the cloud\nstorage cache cannot provide any guarantees about file contents.\nFiles downloaded from the cloud storage cache using the kachery client libraries are always hash-verified (an exception is thrown if the hash is not as expected).\nA malicious actor could potentially upload invalid or mislabeled data\nto the cloud cache; this would not be caught until the data was downloaded\nby other nodes. To mitigate this possibility, again, ensure that permissions\nare only granted to trustworthy nodes with responsible owners.\n"}, "feeds.md": {"content": "# Feeds\n\nA kachery feed is a collection of append-only logs called subfeeds, and is writeable by one and only one kachery node. Unlike kachery files, which are referred to by [content URIs](./content-uris.md), and are guaranteed never to change, subfeeds are meant to grow over time, and therefore cannot be referenced by their content. Instead they are referenced by a feed URI of the form:\n\n```\nfeed://d83129c270a87995917e26eb3cea504d2431681fe66cc3bfd02083ef7bfaecdb/primes\n```\n\nThe 64-character hex string in this URI is the feed ID and represents a public signing key generated using the [Ed25519](https://en.wikipedia.org/wiki/EdDSA) algorithm. The corresponding private key is stored internally on the node that created the feed and is kept a secret. In order to be considered valid, messages in this feed must be signed using the private key. The authenticity of the messages may subsequently be verified by any node using the public key. As long as the private key is not compromised (and the writer node obeys the rules of kachery feeds), we can trust that this feed URI points to a single well-defined collection of messages.\n\nThe name \"primes\" at the end of this URI refers to a subfeed, which in this case is an append-only log of prime number records. Right now it contains all 168 prime numbers less than 1000 in sequence, but in the future it may contain more records. Unless the secret key is compromised, it is guaranteed that the first 168 records of this feed will always remain the same.\n\nThe above feed was created using the follow script:\n\n```python\nfrom typing import List, Set\nimport kachery_client as kc\n\ndef prime_sieve(n: int):\n    is_composite: Set[int] = set()\n    primes: List[int] = []\n    for j in range(2, n + 1):\n        if not j in is_composite:\n            primes.append(j)\n            for k in range(j * j, n + 1, j):\n                is_composite.add(k)\n    return primes\n\nfeed = kc.create_feed()\nsubfeed = feed.load_subfeed('primes')\n\nN = 1000\nprimes = prime_sieve(N)\nfor i, p in enumerate(primes):\n    subfeed.append_message({\n        'p': p,\n        'n': i + 1\n    })\n\nprint(f'Found {len(primes)} prime numbers less than {N}')\nprint(subfeed.uri)\n\n# Output:\n# Found 168 prime numbers less than 1000\n# feed://d83129c270a87995917e26eb3cea504d2431681fe66cc3bfd02083ef7bfaecdb/primes\n```\n\nTo retrieve these messages on the same node or on a different node with access, run the following:\n\n```python\nimport kachery_client as kc\n\nsubfeed = kc.load_subfeed('feed://d83129c270a87995917e26eb3cea504d2431681fe66cc3bfd02083ef7bfaecdb/primes')\nmessages = subfeed.get_next_messages()\nfor msg in messages:\n    print(f'Prime {msg[\"n\"]}: {msg[\"p\"]}')\n\n# Output:\n# Prime 1: 2\n# Prime 2: 3\n# Prime 3: 5\n# ...\n# Prime 168: 997\n```\n\n## Examples of feeds\n\nFeeds provide\na permanent public record of any process that generates information with\na natural chronological ordering. Examples include data collected in a series, the\npath of values in parameters of a machine learning model, the curation\nactions applied to a data set by a lab researcher, a series of financial\ntransactions, the set of placements of opened windows in a user interface, etc.\nThe types of data that fall into this category are very broad.\n\nIn addition to tracking the history of a process, feeds are very useful for\nensuring reproducibility in states between different systems. Because\nthe subfeed acts as a ledger that (when used properly) records every transition\nfrom the initial state to the current state, a remote machine can be synced\nto match the state of the feed originator simply by replaying the steps recorded\nin a subfeed. Moreover, because every message added to a subfeed needs to be signed\nand added by the node that owns it, feeds automatically generate a canonical\nordering for potentially-ambiguous items.\n\n### How are feeds different from files?\n\nSince [kachery files are stored in a content-addressable way](./content-uris.md), they can only\nbe retrieved if the file signature (SHA-1 URI) is known. However, feeds are\nmeant to grow (until finalized)--if their content were known and fixed, they\nwould just be files! Instead, as described in the example above, each feed is assigned a unique identifier, which\nis also the [public part of a public-private keypair](./security.md#Feeds). The\nnode which owns the feed also keeps the private key, and signs each message which\nis officially added to the feed.\n\nFeeds are also distinct from kachery files in that a feed has a specific\nowner node. Files are content-addressable, and thus there is no hierarchy by\nwhich one node's copy would be preferred over another node's--it doesn't matter\nwho contributed the file originally, only what the file contains. For feeds,\nthe node that owns the feed is the authoritative source of its contents, and the only entity that is allowed to append new messages.\n\nFinally, because the messages added to an append-only log within a feed have an inherent ordering (and\nearlier messages are immutable), a node requesting feed data can request the\nmessages after a specific point in time or can monitor new additions in real\ntime. These actions would not make sense in the context of a regular kachery file.\n\n## Feed Structure\n\nAs shown in the above example, each feed is a collection of *subfeeds*, which organize the messages appended to the\nfeed. Messages are always associated with a specific subfeed, rather than the\nfeed as a whole, even if the feed has only one subfeed.\nEach subfeed has its own name (a key used to refer to the subfeed within the feed).\nOn local storage, feeds, subfeeds, and messages are all stored in a SQL database. However, when shared between nodes, subfeed messages are stored as individual objects in a storage bucket.\n\n## Feed Security\n\nAs discussed in the [security model](./security.md#Feeds), each feed is owned by\na specific node which is the sole authority over its contents. The owner node\nretains the private key corresponding to the public key represented by the\nfeed ID, and signs every new message appended to the feed. All nodes which\nobtain feed data can verify its authenticity by checking this signature.\n\nWhile the owner node is the sole author of feed contents, a feed owner can authorize other nodes to submit message to a subfeed via [action tasks](./tasks.md). The\n[SortingView](https://github.com/magland/sortingview) electrophysiology\nvisualization application uses this approach to allow multiple researchers\nto collaborate and contribute their curation actions to a common lab record."}}};
export default x